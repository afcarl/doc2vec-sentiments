{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis với Doc2Vec (Vietnamese)\n",
    "\n",
    "Word2vec là một khám phá phải nói cực kỳ quan trọng. Nó input là corpus và đầu ra là các Vector cho mỗi từ trong copus đó, với độ dài mỗi vector là cố định, thường khoảng 100-300 chiều. \n",
    "\n",
    "Các vector từ này ngoài ra nó còn có tính ngữ nghĩa, và những từ nào tương tự nhau sẽ nằm gần nhau. Nói cách khác, các vector này biểu diễn cách mà ngôn ngữ được tạo ra và cách chúng ta sử dụng chúng mỗi ngày."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ví dụ `v_man` - `v_woman` xấp xỉ bằng `v_king` - `v_queen`, biểu diễn quan hệ \"từ **man** đến **woman** cũng tương tự như **king** đến **queen**\".\n",
    "\n",
    "Quá trình trên trong NLP gọi là **word embedding**. Cách biểu diễn này được sử dụng khá rộng rãi. \n",
    "\n",
    "Từ ý tưởng của word2vec, doc2vec lần đầu được giới thiệu để biểu diễn không chỉ mỗi word vector, mà còn biểu diễn được cả câu (sentences) hay đoạn văn (documents). Từ nền tảng word embedding, thử tưởng tượng bạn có thể wector hóa bất kỳ đoạn văn hay câu văn nào vào các vector fixed-length, và áp dụng các thuật toán classification, cluster, ... cơ bản bất kì trên chúng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bài này mình sẽ sử dụng Doc2vec để classify data Cornell IMDB movie review corpus (http://www.cs.cornell.edu/people/pabo/movie-review-data/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Modules\n",
    "\n",
    "Mình sẽ sử dụng:\n",
    "- Gensim là thư viện nổi tiếng, implement tốt Word2Vec và Doc2Vec. \n",
    "- numpy để thao tác với array.\n",
    "- sklearn để sử dụng Logistic Regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "import numpy as np\n",
    "\n",
    "# Classify\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input format\n",
    "Tải dữ liệu từ movie-review-data\n",
    "\n",
    "> [polarity dataset v2.0](http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz) ( 3.0Mb) (includes README v2.0): 1000 positive and 1000 negative processed reviews. Introduced in Pang/Lee ACL 2004. Released June 2004.\n",
    "\n",
    "Clean bằng cách chuyển hết về lowercase, xóa tất cả các dấu câu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-04-30 10:12:21--  http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\n",
      "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.20\n",
      "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.20|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3127238 (3,0M) [application/x-gzip]\n",
      "Saving to: ‘review_polarity.tar.gz’\n",
      "\n",
      "review_polarity.tar 100%[===================>]   2,98M  2,75MB/s    in 1,1s    \n",
      "\n",
      "2017-04-30 10:12:22 (2,75 MB/s) - ‘review_polarity.tar.gz’ saved [3127238/3127238]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget -O review_polarity.tar.gz http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\n",
    "! tar xzf review_polarity.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Data được bỏ vào 2 thư mục pos/ và neg/. Mình sẽ combine, clean và split thành 4 file.\n",
    "\n",
    "- test-neg.txt: 12500 negative movie reviews from the test data\n",
    "- test-pos.txt: 12500 positive movie reviews from the test data\n",
    "- train-neg.txt: 12500 negative movie reviews from the training data\n",
    "- train-pos.txt: 12500 positive movie reviews from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plot : two teen couples go to a church party , drink and then drive . \r\n",
      "they get into an accident . \r\n",
      "one of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . \r\n",
      "what's the deal ? \r\n",
      "watch the movie and \" sorta \" find out . . . \r\n"
     ]
    }
   ],
   "source": [
    "! head -n 5 txt_sentoken/neg/cv000_29416.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import glob\n",
    "\n",
    "def combine_files(label):\n",
    "    with open(label + \".txt\", 'w') as outfile:\n",
    "        for filename in glob.glob('txt_sentoken/%s/*.txt' % label):\n",
    "            with open(filename, 'rb') as readfile:\n",
    "                shutil.copyfileobj(readfile, outfile)\n",
    "\n",
    "combine_files(\"pos\")\n",
    "combine_files(\"neg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   31783 neg.txt\r\n",
      "   32937 pos.txt\r\n",
      "    9535 test-neg.txt\r\n",
      "    9881 test-pos.txt\r\n",
      "   22247 train-neg.txt\r\n",
      "   23055 train-pos.txt\r\n",
      "  129438 total\r\n"
     ]
    }
   ],
   "source": [
    "# Đếm số dòng\n",
    "! wc -l *.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xóa hết các dấu câu, split thành tập train và test theo tỉ lệ 70/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write training data to: train-pos.txt\n",
      "Write testing data to: test-pos.txt\n",
      "Write training data to: train-neg.txt\n",
      "Write testing data to: test-neg.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "def clean_and_split(label):\n",
    "    with open(label + \".txt\", \"r\") as f:\n",
    "        content = f.read()\n",
    "        content = re.sub(r'([^\\s\\w\\n]|_)+', '', content)\n",
    "        \n",
    "        # Shuffle data và cắt\n",
    "        content = content.split('\\n')\n",
    "        random.shuffle(content)\n",
    "        n_train = int(0.7 * len(content))\n",
    "        \n",
    "        content_train = \"\\n\".join(content[:n_train])\n",
    "        content_test = \"\\n\".join(content[n_train:])\n",
    "        \n",
    "        # Lưu xuống file\n",
    "        with open(\"train-\" + label + \".txt\", \"wb\") as train_f:\n",
    "            print \"Write training data to: %s\" % (\"train-\" + label + \".txt\")\n",
    "            train_f.write(content_train)\n",
    "            train_f.close()\n",
    "        with open(\"test-\" + label + \".txt\", \"wb\") as test_f:\n",
    "            print \"Write testing data to: %s\" % (\"test-\" + label + \".txt\")\n",
    "            test_f.write(content_test)\n",
    "            test_f.close()\n",
    "            \n",
    "clean_and_split(\"pos\")\n",
    "clean_and_split(\"neg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   31783 neg.txt\r\n",
      "   32937 pos.txt\r\n",
      "    9535 test-neg.txt\r\n",
      "    9881 test-pos.txt\r\n",
      "   22247 train-neg.txt\r\n",
      "   23055 train-pos.txt\r\n",
      "  129438 total\r\n"
     ]
    }
   ],
   "source": [
    "! wc -l *.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as taran cradles him in his arms  though  gurgi stirshe is not dead after all  \r\n",
      "based on the mighty successful musical from broadway  grease was followed in 1980 with the less stellar grease 2  6  510   \r\n",
      "disaster is sure to strike  \r\n",
      "synopsis  bobby garfield  yelchin  lives in a small town with his mirthless widowed mother  hope davis   \r\n"
     ]
    }
   ],
   "source": [
    "! head -n 4 test-pos.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feeding Data to Doc2Vec\n",
    "\n",
    "Doc2Vec nhận input đầu vào là class `LabeledLineSentence`. Bởi về Doc2Vec khác Word2Vec chỗ là nó (Doc2Vec) train cả Doc_Id chung với các word. \n",
    "\n",
    "![](https://silvrback.s3.amazonaws.com/uploads/7b02d9b7-20e3-43f8-bed1-e96146611456/sentiment_02_large.png)\n",
    "\n",
    "Doc2Vec vừa vector hóa từng từ, mà còn xác định vector cả document (đoạn văn) chứa các từ ấy.\n",
    "\n",
    "Chúng ta phải format data dưới dạng \n",
    "\n",
    "```\n",
    "[['word1', 'word2', 'word3', 'lastword'], ['label1']]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, sources):\n",
    "        self.sources = sources\n",
    "        \n",
    "        flipped = {}\n",
    "        \n",
    "        # make sure that keys are unique\n",
    "        for key, value in sources.items():\n",
    "            if value not in flipped:\n",
    "                flipped[value] = [key]\n",
    "            else:\n",
    "                raise Exception('Non-unique prefix encountered')\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    yield LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no])\n",
    "    \n",
    "    def to_array(self):\n",
    "        self.sentences = []\n",
    "        for source, prefix in self.sources.items():\n",
    "            with utils.smart_open(source) as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    self.sentences.append(LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))\n",
    "        return self.sentences\n",
    "    \n",
    "    def total_examples(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def sentences_perm(self):\n",
    "        return np.random.permutation(self.sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LabeledLineSentence` đơn giản sẽ truyền vào 1 `dict`, key là tên file và value là **prefix** của doc_id. Nên đặt prefix khác nhau để tránh lỗi trùng lặp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sources = {'test-neg.txt':'TEST_NEG', 'test-pos.txt':'TEST_POS', 'train-neg.txt':'TRAIN_NEG', 'train-pos.txt':'TRAIN_POS'}\n",
    "\n",
    "sentences = LabeledLineSentence(sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sentences` sẽ như sau:\n",
    "\n",
    "```\n",
    "[TaggedDocument(words=[u'once', u'mccabe', u'escapes', u'the', u'film', u'becomes', u'the', u'fugitive', u'in', u'reverse', u'and', u'with', u'no', u'thrills'], tags=['TRAIN_NEG_0']),\n",
    " TaggedDocument(words=[u'it', u'works', u'as', u'a', u'dry', u'comedy', u'which', u'it', u'does', u'not', u'overplay'], tags=['TRAIN_NEG_1']),\n",
    " TaggedDocument(words=[u'you', u'may', u'have', u'noticed', u'that', u'i', u'opted', u'not', u'to', u'describe', u'an', u'iota', u'of', u'mi2s', u'plot'], tags=['TRAIN_NEG_2']), ...\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model \n",
    "\n",
    "### Building the Vocabulary Table\n",
    "Doc2vec bắt chúng ta phải xây dựng voca table. `model.build_vocab` sẽ nhận tham số là `LabeledLineSentence`. \n",
    "\n",
    "Mình sẽ giải thích sơ các tham số của các dòng code bên dưới:\n",
    "* min_count: bỏ qua tất cả các từ có tần số xuất hiện nhỏ hơn `min_count`. Chúng ta set = 1, vì mỗi sentence labels chỉ xuất hiện 1 lần, nếu để giá trị cao hơn thì tất cả các labels sẽ bị xóa.\n",
    "* window: khoảng cách tối đa giữa từ hiện tại và từ predicted. Doc2Vec sử dụng skip-gram model, vậy đây cũng chính là window size của skip-gram model.\n",
    "* size: Số chiều của vector. 100 là ngon rồi, ai nhiều tài nguyên có thể set lên 300-400.\n",
    "* sample: threshold để cấu hình higher-frequency words randomly downsampled.\n",
    "* workers: số worker để training. Nên để bằng số core của cpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec(min_count=1, window=10, size=100, sample=1e-4, negative=5, workers=7)\n",
    "model.build_vocab(sentences.to_array())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Doc2Vec\n",
    "\n",
    "OK bây giờ bắt đầu train model. Model sẽ càng tốt nếu chúng ta train nhiều lần và\n",
    "mỗi lần thứ tự các sentences là khác nhau - hàm `sentences_perm()` của `LabeledLineSentences` có tác dụng đó.\n",
    "\n",
    "Chúng ta train đi train lại khoảng 10 lần. Ai có thời gian thì 20 hoặc 30.\n",
    "Tốn khoảng 10 phút."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-11:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python2.7/threading.py\", line 754, in run\n",
      "    self.__target(*self.__args, **self.__kwargs)\n",
      "  File \"/home/lvduit08/.local/lib/python2.7/site-packages/gensim/models/word2vec.py\", line 853, in job_producer\n",
      "    sentence_length = self._raw_word_count([sentence])\n",
      "  File \"/home/lvduit08/.local/lib/python2.7/site-packages/gensim/models/doc2vec.py\", line 722, in _raw_word_count\n",
      "    return sum(len(sentence.words) for sentence in job)\n",
      "  File \"/home/lvduit08/.local/lib/python2.7/site-packages/gensim/models/doc2vec.py\", line 722, in <genexpr>\n",
      "    return sum(len(sentence.words) for sentence in job)\n",
      "AttributeError: 'numpy.ndarray' object has no attribute 'words'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.train(sentences.sentences_perm(), total_examples=sentences.total_examples(), epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
